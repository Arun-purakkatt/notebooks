{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time risk with atoti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In our example we will create a real-time risk dashboard using atoti.\n",
    "\n",
    "We will consolidate various sensitivities for an equity options portfolio as a data example. We are going to simulate a live feed of market quotes and use QuantLib to recalculate sensitivities on the fly, stream updated risk numbers to an atoti-powered in-memory cube which will update our risk dashboard.\n",
    "\n",
    "Have a look at the GIF of the running atoti application below. On the left, we have a real-time market data feed which is a replay of historical stock prices. On the bottom, there are our trades and on the right, we have risk data continuously re-calculated and re-aggregated at various levels. Every time market data is updated, a chain of actions is performed automatically, resulting in an update to the risk summary table. With this in mind, let’s dive deep into the details!\n",
    "\n",
    "<img src=\"./app-preview.gif\" alt=\"Real-time risk app preview\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# atoti session\n",
    "\n",
    "The first step is to create atoti session for an in-memory data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atoti as tt\n",
    "\n",
    "from atoti.config import create_config\n",
    "\n",
    "config = create_config(\n",
    "    metadata_db=\"./metadata.db\", max_memory=\"8G\", sampling_mode=tt.sampling.FULL\n",
    ")\n",
    "session = tt.create_session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# atoti for real-time update\n",
    "\n",
    "As we discussed, we have two live feeds in our app - a market data feed simulating the tick stock price time-series which is then producing a feed of risk numbers by recalculating option greeks with a Quantlib function. You can easily replace them with your own real time data sources - for example, live trade feeds, sensitivity updates, stress-testing numbers, etc. \n",
    "\n",
    "at the time of writing, atoti provides **three ways to implement real-time update**: \n",
    "\n",
    "- the first is by using the **reactive datastores** - it tells atoti to observe a file or a folder and fetch updates as soon as there are any, \n",
    "- the second is the **append** command that pushes new data into a data store forcing re-calculation. Both methods are illustrated below,\n",
    "- the third is to let a data store listen to a **kafka feed**.\n",
    "\n",
    "**Data visualization** in the atoti app supports the “continuous queries” feature, enabling widgets to automatically refresh the results every time a single contribution changes in the data store. You can toggle between “Turn on real-time”, “Refresh periodically” and “Pause query” for each widget in the atoti app, as shown in this picture:\n",
    "\n",
    "\n",
    "<img src=\"./widget-query-mode.png\" alt=\"Widget query mode\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access atoti dashboards open this URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# atoti data cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a cube with two datastores: one for the market data and one for the risk metrics. \n",
    "\n",
    "atoti provides two methods for the real-time update: the first is by using the reactive datastores - it tells atoti to observe a file or a folder and fetch updates as soon as there are any, and the second is the “append” command that pushes new data into a data store forcing re-calculation. \n",
    "\n",
    "We are creating the `sensitivity_store` and connecting it to a source, in our case a csv file on disc, by setting the parameter `watch` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities_store = session.read_csv(\n",
    "    \"risk_data.csv\",\n",
    "    keys=[\"AsOfDate\", \"TradeId\", \"RiskFactor\"],\n",
    "    store_name=\"Sensitivities\",\n",
    "    types={\"TradeId\": tt.types.STRING, \"AsOfDate\": tt.types.LOCAL_DATE},\n",
    "    watch=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `watch = True` makes the `sensitivities_store` fetch the updates from the file on the disk automatically when it changes. The parameter `watch` is available for `read_csv` and `read_parquet` functions, as described in the atoti [doc](https://docs.atoti.io/0.4.1/lib/atoti.html?highlight=watch#atoti.session.Session.read_csv).\n",
    "\n",
    "\n",
    "To implement real-time instrument prices, we’ll be pushing new values using the datastore `append` command. In the following cell, we're creating the `market_data_store`. As you probably noticed, we are not setting the `watch` parameter as in the previous example, as it defaults to `False`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_store = session.read_csv(\n",
    "    \"market_data.csv\",\n",
    "    keys=[\"AsOfDate\", \"Instrument\"],\n",
    "    store_name=\"Market Data Store\",\n",
    "    types={\n",
    "        \"AsOfDate\": tt.types.LOCAL_DATE,\n",
    "        \"Instrument\": tt.types.STRING,\n",
    "        \"Quote\": tt.types.DOUBLE,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any time, we can use the `append` command to inject new records, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_data_store.append((\"2020-05-01\", \"AAPL\", 310.0))\n",
    "market_data_store.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `join` will link the `sensitivities_store` and the `market_data_store` and allow displaying sensitivities next to quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivities_store.join(\n",
    "    market_data_store, mapping={\"AsOfDate\": \"AsOfDate\", \"RiskFactor\": \"Instrument\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to run the `create_cube` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube = session.create_cube(sensitivities_store, \"Sensitivities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start exploring the data in the cube. The following command serves for a quick inline atoti visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "atoti": {
     "state": {
      "name": "",
      "type": "container",
      "value": {
       "body": {
        "configuration": {
         "tabular": {
          "addButtonFilter": "numeric",
          "cellRenderers": [
           "tree-layout"
          ],
          "columnsGroups": [
           {
            "captionProducer": "firstColumn",
            "cellFactory": "kpi-status",
            "selector": "kpi-status"
           },
           {
            "captionProducer": "firstColumn",
            "cellFactory": "lookup",
            "selector": "lookup"
           },
           {
            "captionProducer": "expiry",
            "cellFactory": "expiry",
            "selector": "kpi-expiry"
           },
           {
            "captionProducer": "columnMerge",
            "cellFactory": {
             "args": {},
             "key": "treeCells"
            },
            "selector": "member"
           }
          ],
          "defaultOptions": {},
          "expansion": {
           "automaticExpansion": true
          },
          "hideAddButton": true,
          "pinnedHeaderSelector": "member",
          "sortingMode": "non-breaking",
          "statisticsShown": true
         }
        },
        "contextValues": {},
        "mdx": "SELECT NON EMPTY {[Measures].[Delta.SUM], [Measures].[MarketValue.SUM], [Measures].[Quote.VALUE]} ON COLUMNS, NON EMPTY Crossjoin(Hierarchize([Hierarchies].[AsOfDate].[AsOfDate].Members), Hierarchize(DrilldownLevel([Hierarchies].[RiskFactor].[ALL].[AllMember]))) ON ROWS FROM [Sensitivities] CELL PROPERTIES VALUE, FORMATTED_VALUE, BACK_COLOR, FORE_COLOR, FONT_FLAGS",
        "ranges": {
         "column": {
          "chunkSize": 50,
          "thresholdPercentage": 0.2
         },
         "row": {
          "chunkSize": 2000,
          "thresholdPercentage": 0.1
         }
        },
        "serverUrl": "",
        "updateMode": "once"
       },
       "containerKey": "pivot-table",
       "showTitleBar": false,
       "style": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "cube.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining the cube\n",
    "\n",
    "The `create_cube` command triggered the creation of default measures and dimensions, which can be further customized and refined, we are creating variables to access their collections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = cube.measures\n",
    "lvl = cube.levels\n",
    "h = cube.hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell **hides the created measures** for the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m[\"MarketValue.MEAN\"].visible = False\n",
    "m[\"Delta.MEAN\"].visible = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is making the date dimension slicing, so that there's no summation across dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: why am I getting UI error \"c is undefined\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[\"AsOfDate\"].slicing = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will sort dates in desc order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl[\"AsOfDate\"].comparator = tt.comparator.DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the atoti documentation to learn about other ways to refine your cube, in particular about creating custom aggregation functions: [New measures](https://docs.atoti.io/0.4.1/tutorial/01-Basics.html#New-measures)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching risk data - trade attributes and multi-level book structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the analysis, we can enrich the metrics with additional attributes such as risk factors, regions, sectors, trade typologies, etc. \n",
    "\n",
    "In this example, we are adding trade attribute data and linking trades to a multi-level booking hierarchy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_attributes = session.read_csv(\n",
    "    \"trade_attributes.csv\",\n",
    "    types={\"TradeId\": tt.types.STRING, \"Strike\": tt.types.STRING},\n",
    "    keys=[\"TradeId\"],\n",
    "    store_name=\"Trade Attributes\",\n",
    ")\n",
    "\n",
    "sensitivities_store.join(trade_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_hierarchy = session.read_pandas(\n",
    "    pd.DataFrame(\n",
    "        data={\n",
    "            \"Book\": [\n",
    "                \"EQ_VOL_HED\",\n",
    "                \"EQ_STRUCT\",\n",
    "                \"EQ_LARG_DM\",\n",
    "                \"EQ_WAREHOU\",\n",
    "                \"EQ_SMAL_EM\",\n",
    "            ],\n",
    "            \"Desk\": [\n",
    "                \"Structuring\",\n",
    "                \"Structuring\",\n",
    "                \"Equity Trading\",\n",
    "                \"Equity Trading\",\n",
    "                \"Equity Trading\",\n",
    "            ],\n",
    "            \"Business Unit\": [\n",
    "                \"Equities\",\n",
    "                \"Equities\",\n",
    "                \"Equities\",\n",
    "                \"Equities\",\n",
    "                \"Equities\",\n",
    "            ],\n",
    "        }\n",
    "    ),\n",
    "    keys=[\"Book\"],\n",
    "    store_name=\"Book Hierarchy\",\n",
    ")\n",
    "\n",
    "trade_attributes.join(book_hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, the **attributes do not have to be static**. They can be refreshed as soon as data arrives. For example, by adding a “Trade status” into the `trade_attributes` store I could use it to tag trades as “Terminated” and filter them out from the view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we are creating a hierarchy representing ogranisational structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h[\"Portfolio Structure\"] = {\n",
    "    \"Business Unit\": lvl[\"Business Unit\"],\n",
    "    \"Desk\": lvl[\"Desk\"],\n",
    "    \"Book\": lvl[\"Book\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand and collapse data along the multi-level hierarchy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "atoti": {
     "state": {
      "name": "",
      "type": "container",
      "value": {
       "body": {
        "configuration": {
         "tabular": {
          "addButtonFilter": "numeric",
          "cellRenderers": [
           "tree-layout"
          ],
          "columnsGroups": [
           {
            "captionProducer": "firstColumn",
            "cellFactory": "kpi-status",
            "selector": "kpi-status"
           },
           {
            "captionProducer": "firstColumn",
            "cellFactory": "lookup",
            "selector": "lookup"
           },
           {
            "captionProducer": "expiry",
            "cellFactory": "expiry",
            "selector": "kpi-expiry"
           },
           {
            "captionProducer": "columnMerge",
            "cellFactory": {
             "args": {},
             "key": "treeCells"
            },
            "selector": "member"
           }
          ],
          "defaultOptions": {},
          "expansion": {
           "automaticExpansion": true
          },
          "hideAddButton": true,
          "pinnedHeaderSelector": "member",
          "sortingMode": "non-breaking",
          "statisticsShown": true
         }
        },
        "contextValues": {},
        "mdx": "SELECT NON EMPTY Hierarchize(DrilldownLevel([Hierarchies].[Portfolio Structure].[ALL].[AllMember])) ON ROWS, NON EMPTY [Measures].[Delta.SUM] ON COLUMNS FROM [Sensitivities] CELL PROPERTIES VALUE, FORMATTED_VALUE, BACK_COLOR, FORE_COLOR, FONT_FLAGS",
        "ranges": {
         "column": {
          "chunkSize": 50,
          "thresholdPercentage": 0.2
         },
         "row": {
          "chunkSize": 2000,
          "thresholdPercentage": 0.1
         }
        },
        "serverUrl": "",
        "updateMode": "once"
       },
       "containerKey": "pivot-table",
       "showTitleBar": false,
       "style": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "cube.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start real-time sources\n",
    "\n",
    "In the following section I'm emulating a market data feed, which is triggering portfolio repricing. \n",
    "\n",
    "A real pricing library is mocked up by a function computing greeks using a QuantLib implementation of the Black-Scholes formulae. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment these lines if you wish to install QuantLib\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install QuantLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pricing_engine_emulator import reprice_trade, reprice_portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a dashboard via this link - you will see the most recent portfolio summary. The widgets are switched to the \"real-time\" query mode, but the data is not updating just yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.url + \"/#/dashboard/8b4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the market data updates, I am going to replay historical stock prices available from Yahoo Finance. This code snippet downloads historical data and pushes new tick updates into a queue. The publisher waits for the `should_publish` event, which is triggered in the subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, queue\n",
    "\n",
    "should_publish = threading.Event()\n",
    "\n",
    "spot_update_queue = queue.Queue()\n",
    "tick_data = pd.read_csv(\n",
    "    \"stock_price_tick_data.csv\", index_col=\"AsOfDate\", parse_dates=[\"AsOfDate\"]\n",
    ")\n",
    "positions = pd.read_csv(\"trade_attributes.csv\", parse_dates=[\"Maturity\"])\n",
    "\n",
    "\n",
    "def start_spot_publisher():\n",
    "    # The publisher is replaying tick_data.\n",
    "    # It waits for the should publish event (see subsequent cells to trigger).\n",
    "    \n",
    "    current_time = -1\n",
    "    for idx, row in tick_data.iterrows():\n",
    "        next_time = idx\n",
    "        if current_time != -1:\n",
    "            dt = (next_time - current_time).total_seconds() / 100.0\n",
    "            time.sleep(dt)\n",
    "        current_time = idx\n",
    "        print(\"\\rawaiting for publishing update \" + str(idx), end='')        \n",
    "        should_publish.wait()\n",
    "        print(\"\\rpublishing update \" + str(idx), end='')        \n",
    "        spot_update_queue.put((idx, row.to_dict()))\n",
    "    spot_update_queue.join()\n",
    "    print(\"All work completed\")\n",
    "\n",
    "\n",
    "def start_spot_update_listener():\n",
    "    print(\"Real time updates started\")\n",
    "    while True:\n",
    "        calc_date, spot_prices_by_ticker = spot_update_queue.get()\n",
    "\n",
    "        \"\"\"\n",
    "        As discussed above, I'm illustrating the \"reactive\" data store \n",
    "        by configuring it with the parameter watch set to True.\n",
    "        With the following code I'm updating the file on disc:\n",
    "        \"\"\"\n",
    "        reprice_portfolio(spot_prices_by_ticker, positions, calc_date).to_csv(\n",
    "            \"risk_data.csv\", index=False\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        The second example of real-time implementation - is injecting the data \n",
    "        directly into a datastore.\n",
    "        With the following code I'm appending the new stock prices to the market data store.\n",
    "        \"\"\"\n",
    "        for i in list(spot_prices_by_ticker.items()):\n",
    "            if not pd.isnull(i[1]):\n",
    "                print((calc_date.strftime(\"%Y-%m-%d\"), i[0], i[1]))\n",
    "                market_data_store.append((calc_date.strftime(\"%Y-%m-%d\"), i[0], i[1]))\n",
    "\n",
    "        spot_update_queue.task_done()\n",
    "\n",
    "\n",
    "listener_thread = threading.Thread(target=start_spot_update_listener, daemon=True)\n",
    "publisher_thread = threading.Thread(target=start_spot_publisher, daemon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Run the next cell to start the real-time feeds - and see the cells in the dashboards blinking if then contributing data changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_thread.start()\n",
    "listener_thread.start()\n",
    "# start publishing\n",
    "should_publish.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pause publishing\n",
    "should_publish.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kafka feed with a custom deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above real-time feeds are illustrating the **reactive** data store and the **append** command. Now let's have a quick look at enabling a **kafka feed** for the trades store.\n",
    "\n",
    "Let's start a kafka server as desribed here: [https://kafka.apache.org/quickstart](https://kafka.apache.org/quickstart).\n",
    "\n",
    "After starting the ZooKeeper and kafka servers, i've created a topic for trades:\n",
    "\n",
    "```\n",
    "bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic trades\n",
    "\n",
    "```\n",
    "Let's point the `trades_datastore` to the kafka feed. atoti will consume the arriving trades data, and the widgets will refresh automatically (need to toggle them to real-time mode).\n",
    "\n",
    "I launched kafka with the default parameters, hence the `bootstrap.server` is `localhost:9092`.\n",
    "\n",
    "For a complete list of the atoti's `load_kafka` parameters, please refer to the doc [load_kafka](https://docs.atoti.io/0.4.2/lib/atoti.html?highlight=kafka#atoti.store.Store.load_kafka). In this example, I will leave use the defaults for the `batch_duration` and `consumer_config` and create a custom serializer - that will skip certain fields from the trade tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = trade_attributes.columns\n",
    "def trade_enricher(record: str):\n",
    "    print(record)\n",
    "    values = record.split(\", \")\n",
    "    fact = {}\n",
    "    for index, column in enumerate(columns):\n",
    "        fact[column] = values[index]\n",
    "    # Examaple of custom deserializer\n",
    "#     fact['update.datetime'] = datetime.now()\n",
    "    return fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_attributes.load_kafka(bootstrap_server =\"localhost:9092\", topic = \"trades\", group_id = \"atoti\", deserializer=tt.kafka.create_deserializer(trade_enricher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment these lines if you wish to install kafka-python\n",
    "# import sys\n",
    "# !conda install  -c conda-forge --yes --prefix {sys.prefix} kafka-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emulating a published trade\n",
    "import json\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "producer = KafkaProducer(value_serializer=lambda v: json.dumps(v).encode(\"utf-8\"), \n",
    "                         bootstrap_servers=['localhost:9092'],api_version=(0,1,0),\n",
    "                        linger_ms = 0)\n",
    "\n",
    "trd = {\n",
    "    \"TradeId\": \"Trd_TEST\",\n",
    "    \"Ticker\": \"OXY\",\n",
    "    \"Book\": \"EQ_VOL_HED\",\n",
    "    \"Product\": \"EQ_Option\",\n",
    "    \"Quantity\": 45,\n",
    "    \"Strike\": -31.41249430583221,\n",
    "    \"Maturity\": \"2022-10-01\",\n",
    "    \"OptionType\": \"put\",\n",
    "    \"MarketValue\": 89.43425919208067,\n",
    "}\n",
    "\n",
    "l = producer.send(\"trades\", trd)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we can see the Trd_TEST in the real-time dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.url + \"/#/dashboard/8b4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, I have a function that will generate random trades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "def generate_a_new_trade(id):\n",
    "\n",
    "    # this function is generating trades data (random)\n",
    "\n",
    "    trade = {\n",
    "        \"TradeId\": \"Trd_\" + str(id),\n",
    "        \"Ticker\": random.choice([\"AAPL\", \"MSFT\", \"OXY\"]),\n",
    "        \"Book\": random.choice(\n",
    "            [\"EQ_LARG_DM\", \"EQ_SMAL_EM\", \"EQ_STRUCT\", \"EQ_VOL_HED\", \"EQ_WAREHOU\"]\n",
    "        ),\n",
    "        \"Product\": \"EQ_Option\",\n",
    "        \"Quantity\": random.randrange(-100, 100),\n",
    "        \"Strike\": random.uniform(-100, 100),\n",
    "        \"Maturity\": \"2022-10-01\",\n",
    "        \"OptionType\": \"put\"\n",
    "#         ,\n",
    "#         \"MarketValue\": random.uniform(-100, 100),\n",
    "    }\n",
    "\n",
    "    print(\"A new trade generated:\")\n",
    "    print(json.dumps(trade, indent=4))\n",
    "\n",
    "    return trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publishing to kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    new_trade = generate_a_new_trade(i)\n",
    "    \n",
    "    producer.send(\"trades\", new_trade)\n",
    "    print(\"Published.\\n\")\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
